{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:False\"\n",
    "\n",
    "import f90nml\n",
    "import numpy as np\n",
    "from pint import UnitRegistry; AssignQuantity = UnitRegistry().Quantity\n",
    "from QLCstuff2 import getNQLL\n",
    "import reference_solution as refsol\n",
    "from scipy.fft import rfft  #, irfft\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.func import grad, jacrev\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation/pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in GI parameters\n",
    "inputfile = \"GI parameters - Reference limit cycle (for testing).nml\"\n",
    "GI=f90nml.read(inputfile)['GI']\n",
    "nx_crystal = GI['nx_crystal']\n",
    "L = GI['L']\n",
    "D = GI['D']\n",
    "D_units = GI['D_units']\n",
    "D = AssignQuantity(D,D_units)# Compute Nqll_eq\n",
    "NBAR = GI['Nbar']\n",
    "NSTAR = GI['Nstar']\n",
    "\n",
    "# Define t range\n",
    "RUNTIME = 5\n",
    "NUM_T_STEPS = RUNTIME + 1\n",
    "\n",
    "# Define initial conditions\n",
    "Ntot_init_1D = np.ones(nx_crystal)\n",
    "Nqll_init_1D = getNQLL(Ntot_init_1D,NSTAR,NBAR)\n",
    "\n",
    "# Define x, t pairs for training\n",
    "X_QLC = np.linspace(-L,L,nx_crystal)\n",
    "t_points = np.linspace(0, RUNTIME, NUM_T_STEPS)\n",
    "x, t = np.meshgrid(X_QLC, t_points)\n",
    "training_set = torch.tensor(np.column_stack((x.flatten(), t.flatten()))).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_gradients(xs, ys):\n",
    "    \"\"\"\n",
    "    Computes gradients needed to compute collocation point loss. \n",
    "    Expects batched input with requires_grad=TRUE.\n",
    "    Args:\n",
    "        xs: xs[0] = x, xs[1] = t\n",
    "        ys: ys[0] = Ntot, ys[1] = Nqll\n",
    "\n",
    "    Returns:\n",
    "        (dNtot_dt, dNqll_dt, dNqll_dxx)\n",
    "    \"\"\"\n",
    "    # batch_size = len(xs)\n",
    "    xs.requires_grad_(True)  # Enable gradients for input (shape: (batch_size, 2))\n",
    "    # TODO - I'm suspicious of these gradients\n",
    "\n",
    "    # ---- Compute First-Order Derivatives ----\n",
    "    # We'll compute the following:\n",
    "    #   Ntot_t = ∂Ntot/∂t, and\n",
    "    #   Nqll_t = ∂Nqll/∂t\n",
    "    #\n",
    "    # Here, the input is [x, t] so t is at index 1.\n",
    "\n",
    "    # Create grad_outputs tensor matching shape of model output\n",
    "    grad_outputs = torch.zeros_like(ys).to(device)\n",
    "    # print(f\"xs: {xs}, requires grad: {xs.requires_grad}\")\n",
    "    # print(f\"ys: {ys}\")\n",
    "    # print(f\"grad outputs (pre calculation1): {grad_outputs}\")\n",
    "\n",
    "    # 1. Compute gradient of Ntot:\n",
    "    grad_outputs[:, 0] = 1.0  # We want gradients for the first output (Ntot)\n",
    "    #print(f\"grad outputs (pre calculation2): {grad_outputs}\")\n",
    "    grad_Ntot = torch.autograd.grad(\n",
    "        outputs=ys,\n",
    "        inputs=xs,\n",
    "        grad_outputs=grad_outputs,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        materialize_grads=True      # default is false\n",
    "    )[0]\n",
    "    #\n",
    "    # print(f\"grad output: {grad_Ntot}\")\n",
    "    Ntot_t = grad_Ntot[:, 1]   # Extract partial w.r.t. t\n",
    "\n",
    "    # 2. Compute gradient of Nqll:\n",
    "    grad_outputs.zero_()      # Reset grad_outputs to zeros\n",
    "    grad_outputs[:, 1] = 1.0  # Now we want gradients for the second output (Nqll)\n",
    "    grad_Nqll = torch.autograd.grad(\n",
    "        outputs=ys,\n",
    "        inputs=xs,\n",
    "        grad_outputs=grad_outputs,\n",
    "        create_graph=True,      # Needed to compute the second-order derivative\n",
    "        retain_graph=True,      # Retain graph for subsequent derivative computations\n",
    "        materialize_grads=True \n",
    "    )[0]\n",
    "    Nqll_t = grad_Nqll[:, 1]   # Extract partial w.r.t. t\n",
    "\n",
    "    # ---- Compute Second-Order Derivative ----\n",
    "    # Now, compute the second derivative of Nqll with respect to x.\n",
    "    # Note that from grad_Nqll we already have ∂Nqll/∂x:\n",
    "    Nqll_x = grad_Nqll[:, 0]\n",
    "\n",
    "    # Compute the second derivative: d²Nqll/dx²\n",
    "    # We take the gradient of Nqll_x with respect to the initial inputs.\n",
    "    grad_Nqll_x = torch.autograd.grad(\n",
    "        outputs=Nqll_x,\n",
    "        inputs=xs,\n",
    "        grad_outputs=torch.ones_like(Nqll_x),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        materialize_grads=True \n",
    "    )[0]\n",
    "    Nqll_xx = grad_Nqll_x[:, 0]  # Extract ∂²Nqll/∂x² (derivative with respect to x)\n",
    "\n",
    "    # print(\"Gradients returned:\")\n",
    "    # print(f\"Ntot_t: {Ntot_t}\")\n",
    "    # print(f\"Nqll_t: {Nqll_t}\")\n",
    "    # print(f\"Nqll_xx: {Nqll_xx}\")\n",
    "    # Return relevant gradients\n",
    "    return Ntot_t.detach(), Nqll_t.detach(), Nqll_xx.detach()\n",
    "    # TODO - verify why detach() is important here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_misc_params():\n",
    "    # TODO - make sigma I calculation work for user-specified collocation points, ie. for random resampling\n",
    "    # Supersaturation reduction at center\n",
    "    c_r = GI['c_r']\n",
    "\n",
    "    # Thickness of monolayers\n",
    "    h_pr = GI['h_pr']\n",
    "    h_pr_units = GI['h_pr_units']\n",
    "    h_pr = AssignQuantity(h_pr,h_pr_units)\n",
    "    h_pr.ito('micrometer')\n",
    "\n",
    "    # Deposition velocity\n",
    "    nu_kin = GI['nu_kin']\n",
    "    nu_kin_units = GI['nu_kin_units']\n",
    "    nu_kin = AssignQuantity(nu_kin,nu_kin_units)\n",
    "\n",
    "    # Difference in equilibrium supersaturation between microsurfaces I and II\n",
    "    sigma0 = torch.tensor(GI['sigma0']).to(device)\n",
    "\n",
    "    # Supersaturation at facet corner\n",
    "    sigmaI_corner = GI['sigmaI_corner']\n",
    "\n",
    "    # Time constant for freezing/thawing\n",
    "    tau_eq = GI['tau_eq']\n",
    "    tau_eq_units = GI['tau_eq_units']\n",
    "    tau_eq = AssignQuantity(tau_eq,tau_eq_units)\n",
    "\n",
    "    # Compute omega_kin\n",
    "    nu_kin_mlyperus = nu_kin/h_pr\n",
    "    nu_kin_mlyperus.ito('1/microsecond')\n",
    "    omega_kin = torch.tensor(nu_kin_mlyperus.magnitude * tau_eq.magnitude).to(device)\n",
    "\n",
    "    # Compute sigmaI\n",
    "    sigmaI = torch.tensor(sigmaI_corner*(c_r*(X_QLC/L)**2+1-c_r))\n",
    "    # Concatenate a copy of sigmaI for each timestep to prevent shape inconsistencies later\n",
    "    sigmaI = torch.cat([sigmaI]*NUM_T_STEPS).to(device)\n",
    "    \n",
    "    # sigma0, sigmaI, omega_kin = params\n",
    "    return sigma0, sigmaI, omega_kin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1d_solve_ivp_dimensionless(Ntot, Nqll, dNqll_dxx, scalar_params):\n",
    "    \"\"\"\n",
    "    Adapted from QLCstuff2, this function computes the right-hand side of\n",
    "    the two objective functions that make up the QLC system.\n",
    "    \n",
    "    Returns:\n",
    "        [dNtot_dt, dNqll_dt]\n",
    "    \"\"\"\n",
    "    sigma0, sigmaI, omega_kin = scalar_params\n",
    "    \n",
    "    # Diffusion term based on FT\n",
    "    # Dcoefficient1 = np.pi**2 / (L)**2  \n",
    "    # bj_list = rfft(Nqll)\n",
    "    # cj_list = bj_list*J2_LIST\n",
    "    # dy = -Dcoefficient1 * irfft(cj_list)    # This is actually a second derivative...\n",
    "\n",
    "    # Ntot deposition\n",
    "    m = (Nqll - (NBAR - NSTAR))/(2*NSTAR)\n",
    "    sigma_m = (sigmaI - m * sigma0)\n",
    "    dNtot_dt = omega_kin * sigma_m\n",
    "    dNtot_dt += dNqll_dxx # Second derivative acquired elsewhere instead of from FT diffusion term\n",
    "    # NQLL    \n",
    "    dNqll_dt = dNtot_dt - (Nqll - (NBAR - NSTAR*torch.sin(2*np.pi*Ntot)))\n",
    "    \n",
    "    # Package for output\n",
    "    return dNtot_dt, dNqll_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLC_model(xs, params, epochs, epoch, print_every, print_gradients):\n",
    "    \"\"\"Defines QLC model. Acts as collocation point loss function.\n",
    "\n",
    "    Args:\n",
    "        xs: xs[0] = x, xs[1] = t\n",
    "        params: output of get_misc_params()\n",
    "\n",
    "    Returns:\n",
    "        (Ntot-loss, Nqll-loss)\n",
    "    \"\"\"\n",
    "    # model predicts output of training_set as batch\n",
    "    ys = model.forward(xs)\n",
    "    \n",
    "    # Calculate and extract gradients\n",
    "    dNtot_dt, dNqll_dt, dNqll_dxx = compute_loss_gradients(xs, ys)\n",
    "    if (((epoch+1) % print_every) == 0) and print_gradients:\n",
    "        print(f\"Gradients: {dNtot_dt}, {dNqll_dt}, {dNqll_dxx}.\")\n",
    "    \n",
    "\n",
    "    # Compute expected output\n",
    "    Ntot, Nqll = ys[:, 0], ys[:, 1]\n",
    "    #with torch.no_grad():\n",
    "    dNtot_dt_rhs, dNqll_dt_rhs = f1d_solve_ivp_dimensionless(Ntot, Nqll, dNqll_dxx, params)\n",
    "    \n",
    "    # dNtot_dt = Nqll*surface_diff_coefficient + w_kin*sigma_m\n",
    "    # dNqll_dt = dNtot/dt - (Nqll - Nqll_eq)\n",
    "    cat_test = torch.stack((dNtot_dt - dNtot_dt_rhs, dNqll_dt - dNqll_dt_rhs), axis=0)\n",
    "    \n",
    "    # Return loss as tensor of shape (2, len(xs))\n",
    "    # [dNtot_dt - dNtot_dt_rhs, dNqll_dt - dNqll_dt_rhs]\n",
    "    return torch.square(cat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define IcePINN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between nn._ and nn.functional._ is that functional is stateless: \n",
    "# https://stackoverflow.com/questions/63826328/torch-nn-functional-vs-torch-nn-pytorch\n",
    "class IcePINN(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, hidden_layer_size):\n",
    "        super().__init__()\n",
    "        self.fc_in = nn.Linear(2, hidden_layer_size)\n",
    "\n",
    "        self.fc_hidden = nn.ModuleList()\n",
    "        for _ in range(num_hidden_layers-1):\n",
    "            self.fc_hidden.append(nn.Linear(hidden_layer_size, hidden_layer_size))\n",
    "            \n",
    "        self.fc_out = nn.Linear(hidden_layer_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc_in(x))\n",
    "        for layer in self.fc_hidden:\n",
    "            x = F.tanh(layer(x))\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_PINN(model: IcePINN, optimizer, training_set, epochs, print_every, print_gradients=False):\n",
    "\n",
    "    print(f\"Commencing PINN training for {epochs} epochs.\")\n",
    "    # Retrieve miscellaneous params for loss calculation\n",
    "    params = get_misc_params()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # evaluate collocation point loss\n",
    "        loss = QLC_model(training_set, params, epochs, epoch, print_every, print_gradients)\n",
    "\n",
    "        if ((epoch+1) % print_every) == 0:\n",
    "            print(f\"Loss at epoch [{epoch+1}/{epochs}]: Ntot = {torch.sum(loss[0]).item()}, Nqll = {torch.sum(loss[1]).item()}.\")\n",
    "            \n",
    "        # backward and optimize\n",
    "        loss.backward(torch.ones_like(loss)) # Computes loss gradients\n",
    "        optimizer.step() # Adjusts weights accordingly\n",
    "        optimizer.zero_grad() # Zeroes gradients so they don't affect future computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate model and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IcePINN(num_hidden_layers=8, hidden_layer_size=80).to(device)\n",
    "\n",
    "# Initialize model weights with HE initialization\n",
    "def init_HE(m):\n",
    "\t\tif type(m) == nn.Linear:\n",
    "\t\t\tnn.init.kaiming_normal_(m.weight)\n",
    "model.apply(init_HE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing PINN training for 50000 epochs.\n",
      "Loss at epoch [1000/50000]: Ntot = 416.4197447532467, Nqll = 94.53013603193837.\n",
      "Loss at epoch [2000/50000]: Ntot = 62.62763990596765, Nqll = 25.1242108725045.\n",
      "Loss at epoch [3000/50000]: Ntot = 27.606846569329196, Nqll = 3.237237347338591.\n",
      "Loss at epoch [4000/50000]: Ntot = 44.389341465075915, Nqll = 11.761409727795654.\n",
      "Loss at epoch [5000/50000]: Ntot = 83.88388630861203, Nqll = 35.93509465450294.\n",
      "Loss at epoch [6000/50000]: Ntot = 275.59624409060837, Nqll = 68.08752844189573.\n",
      "Loss at epoch [7000/50000]: Ntot = 497.6367500454741, Nqll = 113.96570470022823.\n",
      "Loss at epoch [8000/50000]: Ntot = 481.4249911163278, Nqll = 79.66138078082624.\n",
      "Loss at epoch [9000/50000]: Ntot = 7.504982340077781, Nqll = 8.239047315449213.\n",
      "Loss at epoch [10000/50000]: Ntot = 59.63038006933137, Nqll = 50.358539464634376.\n",
      "Loss at epoch [11000/50000]: Ntot = 2.202062821184125, Nqll = 0.8111125419116947.\n",
      "Loss at epoch [12000/50000]: Ntot = 58.05300472004168, Nqll = 21.10989820631292.\n",
      "Loss at epoch [13000/50000]: Ntot = 45.56342538061806, Nqll = 11.323779738726703.\n",
      "Loss at epoch [14000/50000]: Ntot = 40.17251985266882, Nqll = 9.210297330418026.\n",
      "Loss at epoch [15000/50000]: Ntot = 52.12990911963864, Nqll = 11.913924802712987.\n",
      "Loss at epoch [16000/50000]: Ntot = 102.23766963616664, Nqll = 28.188520650958857.\n",
      "Loss at epoch [17000/50000]: Ntot = 126.21247024592142, Nqll = 35.65389807288089.\n",
      "Loss at epoch [18000/50000]: Ntot = 242.0358564289188, Nqll = 76.55594313730077.\n",
      "Loss at epoch [19000/50000]: Ntot = 270.26234197687063, Nqll = 68.53932495770238.\n",
      "Loss at epoch [20000/50000]: Ntot = 288.8873641767889, Nqll = 65.04879144975163.\n",
      "Loss at epoch [21000/50000]: Ntot = 319.5718009511494, Nqll = 71.86936100618482.\n",
      "Loss at epoch [22000/50000]: Ntot = 1526.707850395579, Nqll = 1521.6700000497065.\n",
      "Loss at epoch [23000/50000]: Ntot = 168.16872465594366, Nqll = 48.46783041410993.\n",
      "Loss at epoch [24000/50000]: Ntot = 120.07859986721894, Nqll = 46.18596277914531.\n",
      "Loss at epoch [25000/50000]: Ntot = 95.51121903709607, Nqll = 22.442118986828714.\n",
      "Loss at epoch [26000/50000]: Ntot = 105.6844340912933, Nqll = 25.898656558157143.\n",
      "Loss at epoch [27000/50000]: Ntot = 0.0765399694786947, Nqll = 0.05865498683005426.\n",
      "Loss at epoch [28000/50000]: Ntot = 0.05161153643663102, Nqll = 0.03410952979651974.\n",
      "Loss at epoch [29000/50000]: Ntot = 0.040771225308823295, Nqll = 0.023543425178996914.\n",
      "Loss at epoch [30000/50000]: Ntot = 0.033946227460400824, Nqll = 0.016515138692732908.\n",
      "Loss at epoch [31000/50000]: Ntot = 0.02983070678927189, Nqll = 0.011853635341266953.\n",
      "Loss at epoch [32000/50000]: Ntot = 0.027567300305487076, Nqll = 0.009477116453200922.\n",
      "Loss at epoch [33000/50000]: Ntot = 0.0049209826617182045, Nqll = 0.1378845776876046.\n",
      "Loss at epoch [34000/50000]: Ntot = 0.031218072983144492, Nqll = 0.012553097916921635.\n",
      "Loss at epoch [35000/50000]: Ntot = 0.02949494009880477, Nqll = 0.011035053950524221.\n",
      "Loss at epoch [36000/50000]: Ntot = 0.029725251613092626, Nqll = 0.011612097854998324.\n",
      "Loss at epoch [37000/50000]: Ntot = 0.024630282994984175, Nqll = 0.00610319816091971.\n",
      "Loss at epoch [38000/50000]: Ntot = 0.024466093152772244, Nqll = 0.006035279933937565.\n",
      "Loss at epoch [39000/50000]: Ntot = 0.02394249755556254, Nqll = 0.00567803660790857.\n",
      "Loss at epoch [40000/50000]: Ntot = 5315.132129043217, Nqll = 1281.7514554459467.\n",
      "Loss at epoch [41000/50000]: Ntot = 11918.136861050638, Nqll = 3895.361177263008.\n",
      "Loss at epoch [42000/50000]: Ntot = 3752.6300308327463, Nqll = 1038.0935544473227.\n",
      "Loss at epoch [43000/50000]: Ntot = 3692.188430918608, Nqll = 863.1439914217849.\n",
      "Loss at epoch [44000/50000]: Ntot = 25665.58365956565, Nqll = 23751.214719221087.\n",
      "Loss at epoch [45000/50000]: Ntot = 3371.2084683571857, Nqll = 818.0086361692383.\n",
      "Loss at epoch [46000/50000]: Ntot = 3359.446995475905, Nqll = 764.2986253639033.\n",
      "Loss at epoch [47000/50000]: Ntot = 12169.123246056666, Nqll = 10891.668112797877.\n",
      "Loss at epoch [48000/50000]: Ntot = 2640.431368988873, Nqll = 1884.318273021829.\n",
      "Loss at epoch [49000/50000]: Ntot = 439.4302722017618, Nqll = 233.88967912383433.\n",
      "Loss at epoch [50000/50000]: Ntot = 408.0504922718294, Nqll = 148.02473351069978.\n"
     ]
    }
   ],
   "source": [
    "train_PINN(model, optimizer, training_set, epochs=50_000, print_every=1_000, print_gradients=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "print('Finished Training')\n",
    "PATH = './models/TestPINN.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model (visually?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_model = IcePINN()\n",
    "# loaded_model.load_state_dict(torch.load(PATH)) # it takes the loaded dictionary, not the path file itself\n",
    "# loaded_model.to(device)\n",
    "# loaded_model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
