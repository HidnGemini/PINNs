{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\jonescode\\PINN-capstone\\icepinn.py:338: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  if tenths_remaining is not 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import f90nml\n",
    "import numpy as np\n",
    "from pint import UnitRegistry; AssignQuantity = UnitRegistry().Quantity\n",
    "import os\n",
    "import reference_solution as refsol\n",
    "from scipy.fft import rfft\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import icepinn as ip\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "print(torch.cuda.device_count())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "device = ip.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in GI parameters\n",
    "inputfile = \"GI parameters - Reference limit cycle (for testing).nml\"\n",
    "GI=f90nml.read(inputfile)['GI']\n",
    "nx_crystal = GI['nx_crystal']\n",
    "L = GI['L']\n",
    "NBAR = GI['Nbar']\n",
    "NSTAR = GI['Nstar']\n",
    "\n",
    "# Define t range (needs to be same as training file)\n",
    "RUNTIME = 5\n",
    "NUM_T_STEPS = RUNTIME + 1\n",
    "#NUM_T_STEPS = RUNTIME*5 + 1\n",
    "\n",
    "# Define initial conditions\n",
    "Ntot_init = np.ones(nx_crystal)\n",
    "Nqll_init = ip.get_Nqll(Ntot_init)\n",
    "\n",
    "# Define x, t pairs for training\n",
    "X_QLC = np.linspace(-L,L,nx_crystal)\n",
    "t_points = np.linspace(0, RUNTIME, NUM_T_STEPS)\n",
    "x, t = np.meshgrid(X_QLC, t_points)\n",
    "training_set = torch.tensor(np.column_stack((x.flatten(), t.flatten()))).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model attributes; instantiate model\n",
    "model_dimensions = torch.tensor([8, 8]).to(device)\n",
    "is_sf_PINN = torch.tensor(False)\n",
    "model1 = ip.IcePINN(\n",
    "\tnum_hidden_layers=model_dimensions[0], \n",
    "\thidden_layer_size=model_dimensions[1],\n",
    "\tis_sf_PINN=is_sf_PINN.item()).to(device)\n",
    "\n",
    "# Attach model attributes as buffers so they can be saved and loaded\n",
    "model1.register_buffer('dimensions', model_dimensions)\n",
    "model1.register_buffer('is_sf_PINN', is_sf_PINN)\n",
    "\n",
    "# Initialize model weights with HE initialization\n",
    "model1.apply(ip.init_HE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model1.parameters(), lr=1e-5)\n",
    "\n",
    "# Define learning rate scheduling scheme\n",
    "scheduler_summed = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=10000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HE_8wide_nodiff\n",
      "torch.Size([1920, 2])\n",
      "torch.Size([1920, 2])\n",
      "torch.Size([1920, 2])\n",
      "IcePINN(\n",
      "  (sml): SinusoidalMappingLayer()\n",
      "  (post_sml): Linear(in_features=24, out_features=8, bias=True)\n",
      "  (sin): SinActivation()\n",
      "  (fc_in): Linear(in_features=2, out_features=8, bias=True)\n",
      "  (post_fc_in): Linear(in_features=8, out_features=8, bias=True)\n",
      "  (fc_hidden): ModuleList(\n",
      "    (0-5): 6 x Linear(in_features=8, out_features=8, bias=True)\n",
      "  )\n",
      "  (fc_out): Linear(in_features=8, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"HE_8wide_nodiff\"\n",
    "print(MODEL_NAME)\n",
    "print(training_set.shape)\n",
    "print(ip.calc_cp_loss(model1, training_set, ip.get_misc_params(), 10, 1, 1, False, False).shape)\n",
    "print(ip.enforced_model(training_set, model1).shape)\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing PINN training on 1920 points for 100000 epochs.\n",
      "Epoch [1k/100k] at 0m 10s: Ntot = 3100.481, Nqll = 4859.067, LR = 1e-05\n",
      "Epoch [2k/100k] at 0m 21s: Ntot = 1687.048, Nqll = 1608.094, LR = 1e-05\n",
      "Epoch [3k/100k] at 0m 30s: Ntot = 1514.784, Nqll = 972.930, LR = 1e-05\n",
      "Epoch [4k/100k] at 0m 40s: Ntot = 1432.528, Nqll = 546.141, LR = 1e-05\n",
      "Epoch [5k/100k] at 0m 51s: Ntot = 1401.759, Nqll = 341.780, LR = 1e-05\n",
      "Epoch [6k/100k] at 1m 2s: Ntot = 1398.842, Nqll = 263.878, LR = 1e-05\n",
      "Epoch [7k/100k] at 1m 12s: Ntot = 1400.294, Nqll = 237.385, LR = 1e-05\n",
      "Epoch [8k/100k] at 1m 22s: Ntot = 1400.903, Nqll = 226.740, LR = 1e-05\n",
      "Epoch [9k/100k] at 1m 32s: Ntot = 1401.767, Nqll = 219.552, LR = 1e-05\n",
      "Epoch [10k/100k] at 1m 42s: Ntot = 1402.802, Nqll = 213.335, LR = 1e-05\n",
      "Training 1/10 complete! Time remaining estimate: 15m 18s\n",
      "Epoch [11k/100k] at 1m 52s: Ntot = 1404.143, Nqll = 207.777, LR = 1e-05\n",
      "Epoch [12k/100k] at 2m 3s: Ntot = 1406.008, Nqll = 202.589, LR = 1e-05\n",
      "Epoch [13k/100k] at 2m 14s: Ntot = 1408.870, Nqll = 198.851, LR = 1e-05\n",
      "Epoch [14k/100k] at 2m 24s: Ntot = 1411.363, Nqll = 196.844, LR = 1e-05\n",
      "Epoch [15k/100k] at 2m 36s: Ntot = 1412.290, Nqll = 195.514, LR = 1e-05\n",
      "Epoch [16k/100k] at 2m 49s: Ntot = 1412.302, Nqll = 194.446, LR = 1e-05\n",
      "Epoch [17k/100k] at 2m 59s: Ntot = 1412.639, Nqll = 194.368, LR = 1e-05\n",
      "Epoch [18k/100k] at 3m 7s: Ntot = 1412.996, Nqll = 194.881, LR = 1e-05\n",
      "Epoch [19k/100k] at 3m 15s: Ntot = 1412.217, Nqll = 194.946, LR = 1e-05\n",
      "Epoch [20k/100k] at 3m 21s: Ntot = 1409.720, Nqll = 194.371, LR = 1e-05\n",
      "Training 2/10 complete! Time remaining estimate: 13m 20s\n",
      "Epoch [21k/100k] at 3m 29s: Ntot = 1412.747, Nqll = 194.754, LR = 1e-05\n",
      "Epoch [22k/100k] at 3m 35s: Ntot = 1430.395, Nqll = 197.999, LR = 1e-05\n",
      "Epoch [23k/100k] at 3m 42s: Ntot = 1479.792, Nqll = 206.786, LR = 1e-05\n",
      "Epoch [24k/100k] at 3m 48s: Ntot = 1647.322, Nqll = 284.277, LR = 1e-05\n",
      "Epoch [25k/100k] at 3m 55s: Ntot = 1473.969, Nqll = 246.431, LR = 1e-05\n",
      "Epoch [26k/100k] at 4m 1s: Ntot = 1532.484, Nqll = 277.032, LR = 1e-05\n",
      "Epoch [27k/100k] at 4m 8s: Ntot = 1564.838, Nqll = 237.395, LR = 1e-05\n",
      "Epoch [28k/100k] at 4m 15s: Ntot = 1556.525, Nqll = 229.292, LR = 1e-05\n",
      "Epoch [29k/100k] at 4m 21s: Ntot = 1555.003, Nqll = 228.668, LR = 1e-05\n",
      "Epoch [30k/100k] at 4m 27s: Ntot = 1562.626, Nqll = 231.447, LR = 1e-05\n",
      "Training 3/10 complete! Time remaining estimate: 10m 23s\n",
      "Epoch [31k/100k] at 4m 34s: Ntot = 1560.364, Nqll = 231.786, LR = 1e-05\n",
      "Epoch [32k/100k] at 4m 40s: Ntot = 1544.375, Nqll = 229.438, LR = 1e-05\n",
      "Epoch [33k/100k] at 4m 47s: Ntot = 1536.609, Nqll = 228.630, LR = 1e-05\n",
      "Epoch [34k/100k] at 4m 53s: Ntot = 1555.614, Nqll = 233.422, LR = 1e-05\n",
      "Epoch [35k/100k] at 4m 59s: Ntot = 1565.647, Nqll = 236.340, LR = 1e-05\n",
      "Epoch [36k/100k] at 5m 6s: Ntot = 1586.932, Nqll = 241.252, LR = 1e-05\n",
      "Epoch [37k/100k] at 5m 12s: Ntot = 1596.507, Nqll = 243.968, LR = 1e-05\n",
      "Epoch [38k/100k] at 5m 18s: Ntot = 1599.794, Nqll = 245.593, LR = 1e-05\n",
      "Epoch [39k/100k] at 5m 25s: Ntot = 1598.668, Nqll = 246.538, LR = 1e-05\n",
      "Epoch [40k/100k] at 5m 32s: Ntot = 1593.774, Nqll = 247.670, LR = 1e-05\n",
      "Training 4/10 complete! Time remaining estimate: 8m 18s\n",
      "Epoch [41k/100k] at 5m 38s: Ntot = 1586.448, Nqll = 250.159, LR = 1e-05\n",
      "Epoch [42k/100k] at 5m 44s: Ntot = 1578.742, Nqll = 255.174, LR = 1e-05\n",
      "Epoch [43k/100k] at 5m 51s: Ntot = 1573.686, Nqll = 262.035, LR = 1e-05\n",
      "Epoch [44k/100k] at 5m 57s: Ntot = 1580.100, Nqll = 272.455, LR = 1e-05\n"
     ]
    }
   ],
   "source": [
    "ip.train_IcePINN(\n",
    "    model=model1, \n",
    "    optimizer=optimizer, \n",
    "    training_set=training_set, \n",
    "    epochs=100_000, \n",
    "    name=MODEL_NAME, \n",
    "    print_every=1_000, \n",
    "    print_gradients=False,\n",
    "    diffusion=False,\n",
    "    LR_scheduler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model2 as best version of model1\n",
    "# model2 = ip.load_IcePINN(MODEL_NAME)\n",
    "# model2.train()\n",
    "# optimizer2 = torch.optim.AdamW(model2.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train again on smaller LR starting from saved checkpoint (best saved model above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ip.train_IcePINN(\n",
    "#     model=model2, \n",
    "#     optimizer=optimizer2, \n",
    "#     training_set=training_set, \n",
    "#     epochs=100_000, \n",
    "#     name=MODEL_NAME+\"_round2\", \n",
    "#     print_every=1_000, \n",
    "#     print_gradients=False,\n",
    "#     diffusion=True,\n",
    "#     LR_scheduler=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
