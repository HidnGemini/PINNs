{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import f90nml\n",
    "import numpy as np\n",
    "from pint import UnitRegistry; AssignQuantity = UnitRegistry().Quantity\n",
    "import os\n",
    "import reference_solution as refsol\n",
    "from scipy.fft import rfft\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import icepinn as ip\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "print(torch.cuda.device_count())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "device = ip.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in GI parameters\n",
    "inputfile = \"GI parameters - Reference limit cycle (for testing).nml\"\n",
    "GI=f90nml.read(inputfile)['GI']\n",
    "nx_crystal = GI['nx_crystal']\n",
    "L = GI['L']\n",
    "NBAR = GI['Nbar']\n",
    "NSTAR = GI['Nstar']\n",
    "\n",
    "# Define t range in MS (needs to be same as training file)\n",
    "RUNTIME = 2\n",
    "NUM_T_STEPS = 100*RUNTIME + 1\n",
    "#NUM_T_STEPS = RUNTIME*5 + 1\n",
    "\n",
    "# Define initial conditions\n",
    "Ntot_init = torch.ones(nx_crystal).to(device)\n",
    "Nqll_init = ip.get_Nqll(Ntot_init)\n",
    "\n",
    "# Define x, t pairs for training\n",
    "X_QLC = np.linspace(-L,L,nx_crystal)\n",
    "t_points = np.linspace(0, RUNTIME, NUM_T_STEPS)\n",
    "x, t = np.meshgrid(X_QLC, t_points)\n",
    "training_set = torch.tensor(np.column_stack((x.flatten(), t.flatten()))).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming Legend\n",
    "\n",
    "(This is just how I was notating my saved models, you can do whatever works for you. Just make sure you remember the hyperparameters & architecture somehow)\n",
    "\n",
    "CL = curriculum learning  \n",
    "SF = SF_Pinn architecture  \n",
    "HE = hard-enforced initial condition    \n",
    "SE = soft-enforced initial condition \n",
    "{n}wide = nodes per FC-layer  \n",
    "nodiff = diffusion term is excluded  \n",
    "LBFGS = LBFGS was used  \n",
    "SGD = SGD with standard momentum  \n",
    "Nesterov = SGD with Nesterov momentum   \n",
    "AdamW was used if optimizer is unspecified  \n",
    "{n}rt = trained on RUNTIME of {n}   \n",
    "{n}x = {n}*RUNTIME + 1 timesteps in training set    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"HE_128wide_nodiff_2rt_100x\"\n",
    "\n",
    "# Hard enforce IC? (soft-enforced otherwise)\n",
    "hard_enforce_IC = True\n",
    "# Curriculum learning? (Only relevant for HE IC)\n",
    "curriculum_learning = False\n",
    "# Pre-load stage 1 model (and use it instead of stage 1 training)?\n",
    "#   - For CR, this will be the pre IC enforced model\n",
    "#   - For models fine-tuned with L-BFGS, this will be the pre L-BFGS model\n",
    "preload = False\n",
    "# Use L-BFGS after initial optimization?\n",
    "LBFGS = False\n",
    "\n",
    "# Define model attributes\n",
    "model_dimensions = torch.tensor([8, 128]).to(device) # [Num hidden layers, Nodes per layer]\n",
    "is_sf_PINN = torch.tensor(False)\n",
    "diffusion = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "model = ip.IcePINN(\n",
    "\tnum_hidden_layers=model_dimensions[0], \n",
    "\thidden_layer_size=model_dimensions[1],\n",
    "\tis_sf_PINN=is_sf_PINN.item()).to(device)\n",
    "\n",
    "# Attach model attributes as buffers so they can be saved and loaded\n",
    "model.register_buffer('dimensions', model_dimensions)\n",
    "model.register_buffer('is_sf_PINN', is_sf_PINN)\n",
    "\n",
    "# Initialize model weights with HE initialization\n",
    "model.apply(ip.init_HE)\n",
    "\n",
    "# # Define learning rate scheduling scheme\n",
    "# scheduler_summed = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#         optimizer, mode='min', factor=0.5, patience=10000\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MODEL_NAME)\n",
    "print(training_set.shape)\n",
    "print(ip.calc_IcePINN_loss(model, training_set, hard_enforce_IC=hard_enforce_IC).shape)\n",
    "print(ip.enforced_model(training_set, model).shape)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if curriculum_learning:\n",
    "    # First, train without IC enforced (if it wasn't pre-loaded)\n",
    "    if not preload:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "        ip.train_IcePINN(\n",
    "            model=model, \n",
    "            optimizer=optimizer, \n",
    "            training_set=training_set, \n",
    "            epochs=100_000, \n",
    "            name=MODEL_NAME, \n",
    "            print_every=1_000,\n",
    "            diffusion=diffusion,\n",
    "            LR_scheduler=None,\n",
    "            enforce_IC=False)\n",
    "    else:\n",
    "        model = ip.load_IcePINN(MODEL_NAME, pre_IC=True)\n",
    "        model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9, nesterov=True)\n",
    "# SGD Nesterov has exploding gradients with LR >=1e-5\n",
    "\n",
    "if curriculum_learning:\n",
    "    # Gradually enforce IC over adjustment_period and keep it enforced\n",
    "    ip.train_IcePINN(\n",
    "        model=model, \n",
    "        optimizer=optimizer, \n",
    "        training_set=training_set, \n",
    "        epochs=200_000, \n",
    "        name=MODEL_NAME, \n",
    "        print_every=1_000,\n",
    "        diffusion=diffusion,\n",
    "        LR_scheduler=None,\n",
    "        enforce_IC=True,\n",
    "        adjustment_period=100_000)\n",
    "\n",
    "else:\n",
    "    if not preload:\n",
    "        # Train normally with IC enforced\n",
    "        ip.train_IcePINN(\n",
    "            model=model, \n",
    "            optimizer=optimizer, \n",
    "            training_set=training_set, \n",
    "            epochs=100_000, \n",
    "            name=MODEL_NAME, \n",
    "            print_every=500,\n",
    "            diffusion=diffusion,\n",
    "            LR_scheduler=None,\n",
    "            enforce_IC=True,\n",
    "            hard_enforce_IC=hard_enforce_IC,\n",
    "            adjustment_period=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model for future training (LBFGS)\n",
    "model = ip.load_IcePINN(MODEL_NAME)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare for L-BFGS optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbfgs_optim = torch.optim.LBFGS(\n",
    "    params=model.parameters(), \n",
    "    lr=0.1,\n",
    "    max_iter=200, \n",
    "    history_size=50\n",
    ")\n",
    "misc_params = ip.get_misc_params()\n",
    "lbfgs_iter_counter = 0\n",
    "lbfgs_print_freq = 1\n",
    "\n",
    "# closure() is called by L-BFGS when you call step() up to max_iter times\n",
    "def closure():\n",
    "    #lbfgs_iter_counter += 1\n",
    "\n",
    "    lbfgs_optim.zero_grad()\n",
    "    loss = ip.calc_IcePINN_loss(model, training_set, misc_params, diffusion, hard_enforce_IC=hard_enforce_IC)\n",
    "    loss.backward(torch.ones_like(loss))\n",
    "\n",
    "    # Gradient clipping to mitigate exploding gradients\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1, norm_type=2)\n",
    "    #nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
    "\n",
    "    #if lbfgs_iter_counter % lbfgs_print_freq == 0:\n",
    "    # sum and print loss\n",
    "    Ntot_loss = torch.sum(loss[:, 0]).item()\n",
    "    Nqll_loss = torch.sum(loss[:, 1]).item()\n",
    "    print(f\"L-BFGS iteration loss: Ntot = {Ntot_loss:.3f}, Nqll = {Nqll_loss:.3f}\")\n",
    "    # {lbfgs_iter_counter}\n",
    "    # Return as a summed scalar loss: required by L-BFGS\n",
    "    return torch.add(Ntot_loss, Nqll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L-BFGS time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LBFGS:    \n",
    "    # lbfgs_iter_counter = 0\n",
    "    # lbfgs_print_freq = 1\n",
    "    epochs = 10\n",
    "    for e in range(epochs):\n",
    "        print(f\"Epoch {e+1}\")\n",
    "        lbfgs_optim.step(closure)\n",
    "\n",
    "    \n",
    "    save_path = './models/'+MODEL_NAME\n",
    "\n",
    "    # Create folder to store model in if necessary\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    \n",
    "    # Save model (not including initial condition wrapper)\n",
    "    # torch.save(model.state_dict(), save_path+'/post_LBFGS_params.pth')\n",
    "    # TODO - update (load) so that it can load LBFGS results\n",
    "    print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(training_set[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train some more if needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ip.train_IcePINN(\n",
    "#     model=model, \n",
    "#     optimizer=optimizer, \n",
    "#     training_set=training_set, \n",
    "#     epochs=100_000, \n",
    "#     name=MODEL_NAME, \n",
    "#     print_every=1_000, \n",
    "#     diffusion=diffusion,\n",
    "#     LR_scheduler=None,\n",
    "#     enforce_IC=True,\n",
    "#     adjustment_period=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
