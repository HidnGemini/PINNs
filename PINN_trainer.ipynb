{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import f90nml\n",
    "import numpy as np\n",
    "from pint import UnitRegistry; AssignQuantity = UnitRegistry().Quantity\n",
    "from QLCstuff2 import getNQLL\n",
    "import reference_solution as refsol\n",
    "from scipy.fft import rfft  #, irfft\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation/pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in GI parameters\n",
    "inputfile = \"GI parameters - Reference limit cycle (for testing).nml\"\n",
    "GI=f90nml.read(inputfile)['GI']\n",
    "nx_crystal = GI['nx_crystal']\n",
    "L = GI['L']\n",
    "D = GI['D']\n",
    "D_units = GI['D_units']\n",
    "D = AssignQuantity(D,D_units)# Compute Nqll_eq\n",
    "NBAR = GI['Nbar']\n",
    "NSTAR = GI['Nstar']\n",
    "\n",
    "# Define t range\n",
    "RUNTIME = 5\n",
    "NUM_T_STEPS = RUNTIME + 1\n",
    "\n",
    "# Define initial conditions\n",
    "Ntot_init_1D = np.ones(nx_crystal)\n",
    "Nqll_init_1D = getNQLL(Ntot_init_1D,NSTAR,NBAR)\n",
    "\n",
    "# Define x, t pairs for training\n",
    "X_QLC = np.linspace(-L,L,nx_crystal)\n",
    "t_points = np.linspace(0, RUNTIME, NUM_T_STEPS)\n",
    "x, t = np.meshgrid(X_QLC, t_points)\n",
    "training_set = np.column_stack((x.flatten(), t.flatten())).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1d_solve_ivp_dimensionless(Ntot, Nqll, dNqll_dxx, scalar_params, sigmaI):\n",
    "    \"\"\"\n",
    "    Adapted from QLCstuff2, this function computes the right-hand side of\n",
    "    the two objective functions that make up the QLC system.\n",
    "    \n",
    "    Returns:\n",
    "        [dNtot_dt, dNqll_dt]\n",
    "    \"\"\"\n",
    "    sigma0, omega_kin = scalar_params\n",
    "\n",
    "    # Diffusion term based on FT\n",
    "    # Dcoefficient1 = np.pi**2 / (L)**2  \n",
    "    # bj_list = rfft(Nqll)\n",
    "    # cj_list = bj_list*J2_LIST\n",
    "    # dy = -Dcoefficient1 * irfft(cj_list)    # This is actually a second derivative...\n",
    "\n",
    "    # Ntot deposition\n",
    "    m = (Nqll - (NBAR - NSTAR))/(2*NSTAR)\n",
    "    sigma_m = (sigmaI - m * sigma0)\n",
    "    dNtot_dt = omega_kin * sigma_m\n",
    "    dNtot_dt += dNqll_dxx\n",
    "\n",
    "    # NQLL    \n",
    "    dNqll_dt = dNtot_dt - (Nqll - (NBAR - NSTAR*tf.sin(2*np.pi*Ntot)))\n",
    "    \n",
    "    # Package for output\n",
    "    return dNtot_dt, dNqll_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLC_model(xs, ys):\n",
    "    \"\"\"Defines QLC model. Acts as collocation point loss function.\n",
    "\n",
    "    Args:\n",
    "        xs: xs[0] = x, xs[1] = t\n",
    "        ys: ys[0] = Ntot, ys[1] = Nqll\n",
    "\n",
    "    Returns:\n",
    "        [Ntot-loss, Nqll-loss]\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO - rewrite to make it so I don't load all this for each epoch?\n",
    "    Ntot, Nqll = ys[:, 0:1], ys[:, 1:]\n",
    "    \n",
    "    # Compute gradients\n",
    "    dNtot_dt = dde.grad.jacobian(ys, xs, i=0, j=1)\n",
    "    dNqll_dt = dde.grad.jacobian(ys, xs, i=1, j=1)\n",
    "    dNqll_dxx = dde.grad.hessian(ys, xs, i=1, j=0)\n",
    "\n",
    "    # Supersaturation reduction at center\n",
    "    c_r = GI['c_r']\n",
    "\n",
    "    # Thickness of monolayers\n",
    "    h_pr = GI['h_pr']\n",
    "    h_pr_units = GI['h_pr_units']\n",
    "    h_pr = AssignQuantity(h_pr,h_pr_units)\n",
    "    h_pr.ito('micrometer')\n",
    "\n",
    "    # Deposition velocity\n",
    "    nu_kin = GI['nu_kin']\n",
    "    nu_kin_units = GI['nu_kin_units']\n",
    "    nu_kin = AssignQuantity(nu_kin,nu_kin_units)\n",
    "\n",
    "    # Difference in equilibrium supersaturation between microsurfaces I and II\n",
    "    sigma0 = GI['sigma0']\n",
    "\n",
    "    # Supersaturation at facet corner\n",
    "    sigmaI_corner = GI['sigmaI_corner']\n",
    "\n",
    "    # Time constant for freezing/thawing\n",
    "    tau_eq = GI['tau_eq']\n",
    "    tau_eq_units = GI['tau_eq_units']\n",
    "    tau_eq = AssignQuantity(tau_eq,tau_eq_units)\n",
    "\n",
    "    # Compute omega_kin\n",
    "    nu_kin_mlyperus = nu_kin/h_pr\n",
    "    nu_kin_mlyperus.ito('1/microsecond')\n",
    "    omega_kin = nu_kin_mlyperus.magnitude * tau_eq.magnitude\n",
    "\n",
    "    # Compute sigmaI\n",
    "    sigmaI = sigmaI_corner*(c_r*(X_QLC/L)**2+1-c_r)\n",
    "    \n",
    "    # Nbar, Nstar, sigma0, omega_kin, deltax = scalar_params\n",
    "    scalar_params = np.asarray([sigma0, omega_kin])\n",
    "\n",
    "    dNtot_dt_rhs, dNqll_dt_rhs = f1d_solve_ivp_dimensionless(Ntot, Nqll, dNqll_dxx, scalar_params, sigmaI)\n",
    "\n",
    "    # Return [Ntot-loss, Nqll-loss]\n",
    "    return [dNtot_dt - dNtot_dt_rhs, # dNtot_dt = Nqll*surface_diff_coefficient + w_kin*sigma_m\n",
    "            dNqll_dt - dNqll_dt_rhs] # dNqll_dt = dNtot/dt - (Nqll - Nqll_eq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define IcePINN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between nn._ and nn.functional._ is that functional is stateless: \n",
    "# https://stackoverflow.com/questions/63826328/torch-nn-functional-vs-torch-nn-pytorch\n",
    "class IcePINN(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, hidden_layer_size):\n",
    "        super().__init__()\n",
    "        self.fc_in = nn.Linear(2, hidden_size)\n",
    "        self.fc_hidden = nn.ModuleList()\n",
    "        for _ in range(num_hidden_layers-1):\n",
    "            self.fc_hidden.append(nn.Linear(hidden_size, hidden_size))\n",
    "        self.fc_out = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc_in(x))\n",
    "        for layer in self.fc_hidden:\n",
    "            x = F.tanh(layer(x))\n",
    "        x = F.linear(self.fc_out(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate model and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = IcePINN(num_hidden_layers=8, hidden_layer_size=80).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_PINN(model: IcePINN, optimizer, epochs, training_set):\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    \n",
    "    # model predicts output of training_set as batch\n",
    "    pred = model.forward(training_set)\n",
    "\n",
    "    # evaluate collocation point loss\n",
    "    \n",
    "\n",
    "    # backward and optimize\n",
    "    loss.backward() # Computes loss gradients\n",
    "    optimizer.step() # Adjusts weights accordingly\n",
    "    optimizer.zero_grad() # Zeroes gradients so they don't affect future computations\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Finished Training')\n",
    "PATH = './models/TestPINN.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model (visually?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = IcePINN()\n",
    "loaded_model.load_state_dict(torch.load(PATH)) # it takes the loaded dictionary, not the path file itself\n",
    "loaded_model.to(device)\n",
    "loaded_model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
