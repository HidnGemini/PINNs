{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import f90nml\n",
    "import numpy as np\n",
    "from pint import UnitRegistry; AssignQuantity = UnitRegistry().Quantity\n",
    "import os\n",
    "import reference_solution as refsol\n",
    "from scipy.fft import rfft\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import icepinn as ip\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "print(torch.cuda.device_count())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "device = ip.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in GI parameters\n",
    "inputfile = \"GI parameters - Reference limit cycle (for testing).nml\"\n",
    "GI=f90nml.read(inputfile)['GI']\n",
    "nx_crystal = GI['nx_crystal']\n",
    "L = GI['L']\n",
    "NBAR = GI['Nbar']\n",
    "NSTAR = GI['Nstar']\n",
    "\n",
    "# Define t range (needs to be same as training file)\n",
    "RUNTIME = 5\n",
    "NUM_T_STEPS = RUNTIME + 1\n",
    "#NUM_T_STEPS = RUNTIME*5 + 1\n",
    "\n",
    "# Define initial conditions\n",
    "Ntot_init = np.ones(nx_crystal)\n",
    "Nqll_init = ip.get_Nqll(Ntot_init)\n",
    "\n",
    "# Define x, t pairs for training\n",
    "X_QLC = np.linspace(-L,L,nx_crystal)\n",
    "t_points = np.linspace(0, RUNTIME, NUM_T_STEPS)\n",
    "x, t = np.meshgrid(X_QLC, t_points)\n",
    "training_set = torch.tensor(np.column_stack((x.flatten(), t.flatten()))).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming Legend\n",
    "\n",
    "CL = curriculum learning  \n",
    "SF = SF_Pinn architecture  \n",
    "HE = hard-enforced initial condition  \n",
    "{n}wide = nodes per FC-layer  \n",
    "nodiff = diffusion term is excluded  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"CL_HE_16wide_nodiff\"\n",
    "\n",
    "# Pre-load non-IC-enforced model?\n",
    "preload = True\n",
    "\n",
    "# Define model attributes\n",
    "model_dimensions = torch.tensor([8, 16]).to(device)\n",
    "is_sf_PINN = torch.tensor(False)\n",
    "diffusion = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IcePINN(\n",
       "  (sml): SinusoidalMappingLayer()\n",
       "  (post_sml): Linear(in_features=48, out_features=16, bias=True)\n",
       "  (sin): SinActivation()\n",
       "  (fc_in): Linear(in_features=2, out_features=16, bias=True)\n",
       "  (post_fc_in): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (fc_hidden): ModuleList(\n",
       "    (0-5): 6 x Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (fc_out): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate model\n",
    "model1 = ip.IcePINN(\n",
    "\tnum_hidden_layers=model_dimensions[0], \n",
    "\thidden_layer_size=model_dimensions[1],\n",
    "\tis_sf_PINN=is_sf_PINN.item()).to(device)\n",
    "\n",
    "# Attach model attributes as buffers so they can be saved and loaded\n",
    "model1.register_buffer('dimensions', model_dimensions)\n",
    "model1.register_buffer('is_sf_PINN', is_sf_PINN)\n",
    "\n",
    "# Initialize model weights with HE initialization\n",
    "model1.apply(ip.init_HE)\n",
    "\n",
    "# Define learning rate scheduling scheme\n",
    "# scheduler_summed = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "#         optimizer, mode='min', factor=0.5, patience=10000\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CL_HE_16wide_nodiff\n",
      "torch.Size([1920, 2])\n",
      "torch.Size([1920, 2])\n",
      "torch.Size([1920, 2])\n",
      "IcePINN(\n",
      "  (sml): SinusoidalMappingLayer()\n",
      "  (post_sml): Linear(in_features=48, out_features=16, bias=True)\n",
      "  (sin): SinActivation()\n",
      "  (fc_in): Linear(in_features=2, out_features=16, bias=True)\n",
      "  (post_fc_in): Linear(in_features=16, out_features=16, bias=True)\n",
      "  (fc_hidden): ModuleList(\n",
      "    (0-5): 6 x Linear(in_features=16, out_features=16, bias=True)\n",
      "  )\n",
      "  (fc_out): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_NAME)\n",
    "print(training_set.shape)\n",
    "print(ip.calc_cp_loss(model1, training_set, ip.get_misc_params()).shape)\n",
    "print(ip.enforced_model(training_set, model1).shape)\n",
    "print(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, train without IC enforced (if it wasn't pre-loaded)\n",
    "if not preload:\n",
    "    optimizer = torch.optim.AdamW(model1.parameters(), lr=1e-5)\n",
    "    ip.train_IcePINN(\n",
    "        model=model1, \n",
    "        optimizer=optimizer, \n",
    "        training_set=training_set, \n",
    "        epochs=50_000, \n",
    "        name=MODEL_NAME, \n",
    "        print_every=1_000,\n",
    "        diffusion=diffusion,\n",
    "        LR_scheduler=None,\n",
    "        enforce_IC=False)\n",
    "else:\n",
    "    model1 = ip.load_IcePINN(MODEL_NAME, pre_IC=True)\n",
    "    model1.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing PINN training on 1920 points for 200000 epochs.\n",
      "IC is enforced with an adjustment period of 100000.\n",
      "Epoch [1k/200k] at 0m 7s: Ntot = 0.012, Nqll = 0.006, LR = 0.0001\n",
      "Epoch [2k/200k] at 0m 14s: Ntot = 0.069, Nqll = 0.020, LR = 0.0001\n",
      "Epoch [3k/200k] at 0m 21s: Ntot = 0.114, Nqll = 0.031, LR = 0.0001\n",
      "Epoch [4k/200k] at 0m 27s: Ntot = 0.112, Nqll = 0.035, LR = 0.0001\n",
      "Epoch [5k/200k] at 0m 34s: Ntot = 0.116, Nqll = 0.034, LR = 0.0001\n",
      "Epoch [6k/200k] at 0m 41s: Ntot = 0.112, Nqll = 0.035, LR = 0.0001\n",
      "Epoch [7k/200k] at 0m 48s: Ntot = 0.133, Nqll = 0.039, LR = 0.0001\n",
      "Epoch [8k/200k] at 0m 54s: Ntot = 1.065, Nqll = 0.273, LR = 0.0001\n",
      "Epoch [9k/200k] at 1m 1s: Ntot = 0.646, Nqll = 0.265, LR = 0.0001\n",
      "Epoch [10k/200k] at 1m 8s: Ntot = 0.785, Nqll = 0.241, LR = 0.0001\n",
      "Epoch [11k/200k] at 1m 14s: Ntot = 0.805, Nqll = 0.217, LR = 0.0001\n",
      "Epoch [12k/200k] at 1m 21s: Ntot = 0.682, Nqll = 0.171, LR = 0.0001\n",
      "Epoch [13k/200k] at 1m 28s: Ntot = 0.657, Nqll = 0.158, LR = 0.0001\n",
      "Epoch [14k/200k] at 1m 34s: Ntot = 0.700, Nqll = 0.196, LR = 0.0001\n",
      "Epoch [15k/200k] at 1m 41s: Ntot = 0.335, Nqll = 0.111, LR = 0.0001\n",
      "Epoch [16k/200k] at 1m 48s: Ntot = 0.179, Nqll = 0.063, LR = 0.0001\n",
      "Epoch [17k/200k] at 1m 54s: Ntot = 0.238, Nqll = 0.063, LR = 0.0001\n",
      "Epoch [18k/200k] at 2m 1s: Ntot = 0.184, Nqll = 0.046, LR = 0.0001\n",
      "Epoch [19k/200k] at 2m 8s: Ntot = 0.148, Nqll = 0.038, LR = 0.0001\n",
      "Epoch [20k/200k] at 2m 14s: Ntot = 0.123, Nqll = 0.031, LR = 0.0001\n",
      "Training 1/10ths complete! Completion estimate: 22m 20s | 20m 6s remaining.\n",
      "Epoch [21k/200k] at 2m 21s: Ntot = 0.114, Nqll = 0.028, LR = 0.0001\n",
      "Epoch [22k/200k] at 2m 27s: Ntot = 0.109, Nqll = 0.026, LR = 0.0001\n",
      "Epoch [23k/200k] at 2m 34s: Ntot = 0.106, Nqll = 0.024, LR = 0.0001\n",
      "Epoch [24k/200k] at 2m 40s: Ntot = 0.103, Nqll = 0.024, LR = 0.0001\n",
      "Epoch [25k/200k] at 2m 47s: Ntot = 0.102, Nqll = 0.024, LR = 0.0001\n",
      "Epoch [26k/200k] at 2m 54s: Ntot = 0.102, Nqll = 0.024, LR = 0.0001\n",
      "Epoch [27k/200k] at 3m 0s: Ntot = 0.104, Nqll = 0.024, LR = 0.0001\n",
      "Epoch [28k/200k] at 3m 6s: Ntot = 0.107, Nqll = 0.025, LR = 0.0001\n",
      "Epoch [29k/200k] at 3m 13s: Ntot = 0.111, Nqll = 0.026, LR = 0.0001\n",
      "Epoch [30k/200k] at 3m 20s: Ntot = 0.125, Nqll = 0.023, LR = 0.0001\n",
      "Epoch [31k/200k] at 3m 26s: Ntot = 0.124, Nqll = 0.029, LR = 0.0001\n",
      "Epoch [32k/200k] at 3m 33s: Ntot = 0.133, Nqll = 0.030, LR = 0.0001\n",
      "Epoch [33k/200k] at 3m 40s: Ntot = 0.142, Nqll = 0.033, LR = 0.0001\n",
      "Epoch [34k/200k] at 3m 46s: Ntot = 2.599, Nqll = 0.617, LR = 0.0001\n",
      "Epoch [35k/200k] at 3m 53s: Ntot = 2.903, Nqll = 0.630, LR = 0.0001\n",
      "Epoch [36k/200k] at 3m 59s: Ntot = 2.906, Nqll = 0.640, LR = 0.0001\n",
      "Epoch [37k/200k] at 4m 6s: Ntot = 3.052, Nqll = 0.678, LR = 0.0001\n",
      "Epoch [38k/200k] at 4m 12s: Ntot = 3.339, Nqll = 0.743, LR = 0.0001\n",
      "Epoch [39k/200k] at 4m 18s: Ntot = 3.700, Nqll = 0.823, LR = 0.0001\n",
      "Epoch [40k/200k] at 4m 25s: Ntot = 4.129, Nqll = 0.918, LR = 0.0001\n",
      "Training 2/10ths complete! Completion estimate: 22m 0s | 17m 36s remaining.\n",
      "Epoch [41k/200k] at 4m 31s: Ntot = 4.635, Nqll = 1.029, LR = 0.0001\n",
      "Epoch [42k/200k] at 4m 38s: Ntot = 5.235, Nqll = 1.160, LR = 0.0001\n",
      "Epoch [43k/200k] at 4m 45s: Ntot = 5.967, Nqll = 1.316, LR = 0.0001\n",
      "Epoch [44k/200k] at 4m 51s: Ntot = 6.825, Nqll = 1.516, LR = 0.0001\n",
      "Epoch [45k/200k] at 4m 58s: Ntot = 7.841, Nqll = 1.753, LR = 0.0001\n",
      "Epoch [46k/200k] at 5m 5s: Ntot = 8.968, Nqll = 2.015, LR = 0.0001\n",
      "Epoch [47k/200k] at 5m 12s: Ntot = 10.240, Nqll = 2.282, LR = 0.0001\n",
      "Epoch [48k/200k] at 5m 19s: Ntot = 11.618, Nqll = 2.591, LR = 0.0001\n",
      "Epoch [49k/200k] at 5m 25s: Ntot = 13.180, Nqll = 2.944, LR = 0.0001\n",
      "Epoch [50k/200k] at 5m 32s: Ntot = 15.005, Nqll = 3.356, LR = 0.0001\n",
      "Epoch [51k/200k] at 5m 39s: Ntot = 17.186, Nqll = 3.850, LR = 0.0001\n",
      "Epoch [52k/200k] at 5m 45s: Ntot = 19.726, Nqll = 4.415, LR = 0.0001\n",
      "Epoch [53k/200k] at 5m 52s: Ntot = 22.585, Nqll = 5.057, LR = 0.0001\n",
      "Epoch [54k/200k] at 5m 59s: Ntot = 25.599, Nqll = 5.865, LR = 0.0001\n",
      "Epoch [55k/200k] at 6m 5s: Ntot = 29.076, Nqll = 6.521, LR = 0.0001\n",
      "Epoch [56k/200k] at 6m 12s: Ntot = 33.083, Nqll = 7.467, LR = 0.0001\n",
      "Epoch [57k/200k] at 6m 18s: Ntot = 36.340, Nqll = 8.493, LR = 0.0001\n",
      "Epoch [58k/200k] at 6m 25s: Ntot = 42.585, Nqll = 9.222, LR = 0.0001\n",
      "Epoch [59k/200k] at 6m 31s: Ntot = 48.013, Nqll = 10.860, LR = 0.0001\n",
      "Epoch [60k/200k] at 6m 38s: Ntot = 54.002, Nqll = 12.239, LR = 0.0001\n",
      "Training 3/10ths complete! Completion estimate: 22m 0s | 15m 24s remaining.\n",
      "Epoch [61k/200k] at 6m 45s: Ntot = 61.696, Nqll = 13.783, LR = 0.0001\n",
      "Epoch [62k/200k] at 6m 51s: Ntot = 69.288, Nqll = 15.631, LR = 0.0001\n",
      "Epoch [63k/200k] at 6m 58s: Ntot = 78.934, Nqll = 17.618, LR = 0.0001\n",
      "Epoch [64k/200k] at 7m 4s: Ntot = 95.131, Nqll = 22.221, LR = 0.0001\n",
      "Epoch [65k/200k] at 7m 11s: Ntot = 107.246, Nqll = 24.817, LR = 0.0001\n",
      "Epoch [66k/200k] at 7m 18s: Ntot = 130.799, Nqll = 29.290, LR = 0.0001\n",
      "Epoch [67k/200k] at 7m 24s: Ntot = 160.537, Nqll = 35.817, LR = 0.0001\n",
      "Epoch [68k/200k] at 7m 31s: Ntot = 238.968, Nqll = 56.491, LR = 0.0001\n",
      "Epoch [69k/200k] at 7m 38s: Ntot = 462.542, Nqll = 104.614, LR = 0.0001\n",
      "Epoch [70k/200k] at 7m 44s: Ntot = 640.640, Nqll = 144.846, LR = 0.0001\n",
      "Epoch [71k/200k] at 7m 51s: Ntot = 768.872, Nqll = 169.930, LR = 0.0001\n",
      "Epoch [72k/200k] at 7m 58s: Ntot = 768.489, Nqll = 171.165, LR = 0.0001\n",
      "Epoch [73k/200k] at 8m 5s: Ntot = 864.331, Nqll = 192.779, LR = 0.0001\n",
      "Epoch [74k/200k] at 8m 11s: Ntot = 1220.297, Nqll = 272.149, LR = 0.0001\n",
      "Epoch [75k/200k] at 8m 18s: Ntot = 1283.587, Nqll = 286.941, LR = 0.0001\n",
      "Epoch [76k/200k] at 8m 24s: Ntot = 1282.112, Nqll = 285.732, LR = 0.0001\n",
      "Epoch [77k/200k] at 8m 31s: Ntot = 1297.924, Nqll = 289.345, LR = 0.0001\n",
      "Epoch [78k/200k] at 8m 37s: Ntot = 1352.085, Nqll = 301.621, LR = 0.0001\n",
      "Epoch [79k/200k] at 8m 44s: Ntot = 1453.748, Nqll = 325.078, LR = 0.0001\n",
      "Epoch [80k/200k] at 8m 51s: Ntot = 1570.003, Nqll = 350.935, LR = 0.0001\n",
      "Training 4/10ths complete! Completion estimate: 22m 0s | 13m 12s remaining.\n",
      "Epoch [81k/200k] at 8m 58s: Ntot = 1686.370, Nqll = 376.647, LR = 0.0001\n",
      "Epoch [82k/200k] at 9m 5s: Ntot = 1826.841, Nqll = 408.145, LR = 0.0001\n",
      "Epoch [83k/200k] at 9m 11s: Ntot = 1979.303, Nqll = 441.870, LR = 0.0001\n",
      "Epoch [84k/200k] at 9m 18s: Ntot = 2139.019, Nqll = 477.355, LR = 0.0001\n",
      "Epoch [85k/200k] at 9m 25s: Ntot = 2350.319, Nqll = 531.751, LR = 0.0001\n",
      "Epoch [86k/200k] at 9m 32s: Ntot = 2574.612, Nqll = 576.006, LR = 0.0001\n",
      "Epoch [87k/200k] at 9m 38s: Ntot = 2873.993, Nqll = 638.517, LR = 0.0001\n",
      "Epoch [88k/200k] at 9m 46s: Ntot = 3199.216, Nqll = 710.395, LR = 0.0001\n",
      "Epoch [89k/200k] at 9m 54s: Ntot = 3648.493, Nqll = 786.321, LR = 0.0001\n",
      "Epoch [90k/200k] at 10m 1s: Ntot = 3976.179, Nqll = 880.221, LR = 0.0001\n",
      "Epoch [91k/200k] at 10m 8s: Ntot = 4737.573, Nqll = 1047.094, LR = 0.0001\n",
      "Epoch [92k/200k] at 10m 14s: Ntot = 5862.168, Nqll = 1296.613, LR = 0.0001\n",
      "Epoch [93k/200k] at 10m 21s: Ntot = 7464.452, Nqll = 1635.122, LR = 0.0001\n",
      "Epoch [94k/200k] at 10m 28s: Ntot = 9759.637, Nqll = 2135.695, LR = 0.0001\n",
      "Epoch [95k/200k] at 10m 34s: Ntot = 12739.026, Nqll = 2772.783, LR = 0.0001\n",
      "Epoch [96k/200k] at 10m 41s: Ntot = 16520.523, Nqll = 3583.477, LR = 0.0001\n",
      "Epoch [97k/200k] at 10m 48s: Ntot = 20932.289, Nqll = 4537.266, LR = 0.0001\n",
      "Epoch [98k/200k] at 10m 55s: Ntot = 25383.796, Nqll = 5438.171, LR = 0.0001\n",
      "Epoch [99k/200k] at 11m 2s: Ntot = 28884.807, Nqll = 6032.935, LR = 0.0001\n",
      "Epoch [100k/200k] at 11m 9s: Ntot = 30272.838, Nqll = 6203.650, LR = 0.0001\n",
      "Training 5/10ths complete! Completion estimate: 22m 10s | 11m 5s remaining.\n",
      "Adjustment period is complete! IC is now being fully enforced.\n",
      "Epoch [101k/200k] at 11m 16s: Ntot = 29294.272, Nqll = 5993.849, LR = 0.0001\n",
      "Epoch [102k/200k] at 11m 23s: Ntot = 28138.881, Nqll = 5675.283, LR = 0.0001\n",
      "Epoch [103k/200k] at 11m 32s: Ntot = 27063.589, Nqll = 5461.256, LR = 0.0001\n",
      "Epoch [104k/200k] at 11m 40s: Ntot = 25974.112, Nqll = 5214.564, LR = 0.0001\n",
      "Epoch [105k/200k] at 11m 47s: Ntot = 25214.061, Nqll = 5101.250, LR = 0.0001\n",
      "Epoch [106k/200k] at 11m 54s: Ntot = 24567.015, Nqll = 4984.934, LR = 0.0001\n",
      "Epoch [107k/200k] at 12m 1s: Ntot = 24019.384, Nqll = 4901.144, LR = 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Restart optimizer\n",
    "optimizer = torch.optim.AdamW(model1.parameters(), lr=1e-4)\n",
    "\n",
    "# Then, gradually enforce IC over adjustment_period and keep it enforced\n",
    "ip.train_IcePINN(\n",
    "    model=model1, \n",
    "    optimizer=optimizer, \n",
    "    training_set=training_set, \n",
    "    epochs=200_000, \n",
    "    name=MODEL_NAME, \n",
    "    print_every=1_000,\n",
    "    diffusion=diffusion,\n",
    "    LR_scheduler=None,\n",
    "    enforce_IC=True,\n",
    "    adjustment_period=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model2 as best version of model1\n",
    "# model2 = ip.load_IcePINN(MODEL_NAME)\n",
    "# model2.train()\n",
    "# optimizer2 = torch.optim.AdamW(model2.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train again on smaller LR starting from saved checkpoint (best saved model above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ip.train_IcePINN(\n",
    "#     model=model1, \n",
    "#     optimizer=optimizer, \n",
    "#     training_set=training_set, \n",
    "#     epochs=100_000, \n",
    "#     name=MODEL_NAME, \n",
    "#     print_every=1_000, \n",
    "#     diffusion=diffusion,\n",
    "#     LR_scheduler=None,\n",
    "#     enforce_IC=True,\n",
    "#     adjustment_period=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
