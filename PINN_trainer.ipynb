{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "1\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import f90nml\n",
    "import numpy as np\n",
    "from pint import UnitRegistry; AssignQuantity = UnitRegistry().Quantity\n",
    "import os\n",
    "import reference_solution as refsol\n",
    "from scipy.fft import rfft\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import icepinn as ip\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "print(torch.cuda.device_count())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "device = ip.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in GI parameters\n",
    "inputfile = \"GI parameters - Reference limit cycle (for testing).nml\"\n",
    "GI=f90nml.read(inputfile)['GI']\n",
    "nx_crystal = GI['nx_crystal']\n",
    "L = GI['L']\n",
    "NBAR = GI['Nbar']\n",
    "NSTAR = GI['Nstar']\n",
    "\n",
    "# Define t range (needs to be same as training file)\n",
    "RUNTIME = 5\n",
    "#NUM_T_STEPS = RUNTIME + 1\n",
    "NUM_T_STEPS = RUNTIME*5 + 1\n",
    "\n",
    "# Define initial conditions\n",
    "Ntot_init = np.ones(nx_crystal)\n",
    "Nqll_init = ip.get_Nqll(Ntot_init)\n",
    "\n",
    "# Define x, t pairs for training\n",
    "X_QLC = np.linspace(-L,L,nx_crystal)\n",
    "t_points = np.linspace(0, RUNTIME, NUM_T_STEPS)\n",
    "x, t = np.meshgrid(X_QLC, t_points)\n",
    "training_set = torch.tensor(np.column_stack((x.flatten(), t.flatten()))).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model attributes; instantiate model\n",
    "model_dimensions = torch.tensor([8, 80]).to(device)\n",
    "is_sf_PINN = torch.tensor(False)\n",
    "model1 = ip.IcePINN(\n",
    "\tnum_hidden_layers=model_dimensions[0], \n",
    "\thidden_layer_size=model_dimensions[0],\n",
    "\tis_sf_PINN=is_sf_PINN.item()).to(device)\n",
    "\n",
    "# Attach model attributes as buffers so they can be saved and loaded\n",
    "model1.register_buffer('dimensions', model_dimensions)\n",
    "model1.register_buffer('is_sf_PINN', is_sf_PINN)\n",
    "\n",
    "# Initialize model weights with HE initialization\n",
    "\n",
    "model1.apply(ip.init_HE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model1.parameters(), lr=0.001)\n",
    "\n",
    "# Define learning rate scheduling scheme\n",
    "scheduler_summed = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=10000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8320, 2])\n",
      "torch.Size([8320, 2])\n",
      "torch.Size([8320, 2])\n"
     ]
    }
   ],
   "source": [
    "print(training_set.shape)\n",
    "print(ip.calc_cp_loss(model, training_set, ip.get_misc_params(), 10, 1, 1, False).shape)\n",
    "print(ip.enforced_model(training_set, model).shape)\n",
    "\n",
    "MODEL_NAME = \"TestPINN_hard_enforced_SF\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing PINN training on 8320 points for 200000 epochs.\n",
      "Epoch [1000/200000]: Ntot = 4605.302, Nqll = 843.847, LR = 0.001\n",
      "Epoch [2000/200000]: Ntot = 4492.581, Nqll = 817.493, LR = 0.001\n",
      "Epoch [3000/200000]: Ntot = 4448.287, Nqll = 838.971, LR = 0.001\n",
      "Epoch [4000/200000]: Ntot = 4408.222, Nqll = 848.948, LR = 0.001\n",
      "Epoch [5000/200000]: Ntot = 4407.769, Nqll = 882.040, LR = 0.001\n",
      "Epoch [6000/200000]: Ntot = 5110.055, Nqll = 1095.239, LR = 0.001\n",
      "Epoch [7000/200000]: Ntot = 5140.427, Nqll = 948.118, LR = 0.001\n",
      "Epoch [8000/200000]: Ntot = 9812.062, Nqll = 1900.371, LR = 0.001\n",
      "Epoch [9000/200000]: Ntot = 15331.307, Nqll = 3249.935, LR = 0.001\n",
      "Epoch [10000/200000]: Ntot = 23906.017, Nqll = 6978.736, LR = 0.001\n",
      "Epoch [11000/200000]: Ntot = 12439.407, Nqll = 2667.248, LR = 0.001\n",
      "Epoch [12000/200000]: Ntot = 15808.532, Nqll = 3486.519, LR = 0.001\n",
      "Epoch [13000/200000]: Ntot = 62.375, Nqll = 13.022, LR = 0.001\n",
      "Epoch [14000/200000]: Ntot = 22.083, Nqll = 1.749, LR = 0.001\n",
      "Epoch [15000/200000]: Ntot = 29.267, Nqll = 6.427, LR = 0.001\n",
      "Epoch [16000/200000]: Ntot = 154.945, Nqll = 23.535, LR = 0.001\n",
      "Epoch [17000/200000]: Ntot = 382.559, Nqll = 23.138, LR = 0.001\n",
      "Epoch [18000/200000]: Ntot = 109.543, Nqll = 12.405, LR = 0.001\n",
      "Epoch [19000/200000]: Ntot = 83.467, Nqll = 21.569, LR = 0.001\n",
      "Epoch [20000/200000]: Ntot = 78.327, Nqll = 18.250, LR = 0.001\n",
      "Epoch [21000/200000]: Ntot = 95.829, Nqll = 19.916, LR = 0.001\n",
      "Epoch [22000/200000]: Ntot = 163.001, Nqll = 23.530, LR = 0.001\n",
      "Epoch [23000/200000]: Ntot = 148.274, Nqll = 22.191, LR = 0.001\n",
      "Epoch [24000/200000]: Ntot = 354.038, Nqll = 21.425, LR = 0.001\n",
      "Epoch [25000/200000]: Ntot = 475.760, Nqll = 25.936, LR = 0.0005\n",
      "Epoch [26000/200000]: Ntot = 532.375, Nqll = 13.816, LR = 0.0005\n",
      "Epoch [27000/200000]: Ntot = 570.278, Nqll = 15.266, LR = 0.0005\n",
      "Epoch [28000/200000]: Ntot = 662.837, Nqll = 23.830, LR = 0.0005\n",
      "Epoch [29000/200000]: Ntot = 661.659, Nqll = 25.405, LR = 0.0005\n",
      "Epoch [30000/200000]: Ntot = 673.315, Nqll = 15.783, LR = 0.0005\n",
      "Epoch [31000/200000]: Ntot = 699.437, Nqll = 8.059, LR = 0.0005\n",
      "Epoch [32000/200000]: Ntot = 127.969, Nqll = 15.121, LR = 0.0005\n",
      "Epoch [33000/200000]: Ntot = 16.242, Nqll = 7.662, LR = 0.0005\n",
      "Epoch [34000/200000]: Ntot = 24.379, Nqll = 9.506, LR = 0.0005\n",
      "Epoch [35000/200000]: Ntot = 101.058, Nqll = 23.305, LR = 0.00025\n",
      "Epoch [36000/200000]: Ntot = 485.892, Nqll = 62.304, LR = 0.00025\n",
      "Epoch [37000/200000]: Ntot = 2052.989, Nqll = 363.774, LR = 0.00025\n",
      "Epoch [38000/200000]: Ntot = 3584.824, Nqll = 609.302, LR = 0.00025\n",
      "Epoch [39000/200000]: Ntot = 3687.940, Nqll = 694.512, LR = 0.00025\n",
      "Epoch [40000/200000]: Ntot = 2833.712, Nqll = 572.816, LR = 0.00025\n",
      "Epoch [41000/200000]: Ntot = 6113.157, Nqll = 1246.927, LR = 0.00025\n",
      "Epoch [42000/200000]: Ntot = 676.380, Nqll = 138.485, LR = 0.00025\n",
      "Epoch [43000/200000]: Ntot = 562.427, Nqll = 114.774, LR = 0.00025\n",
      "Epoch [44000/200000]: Ntot = 38850.370, Nqll = 4766.649, LR = 0.00025\n",
      "Epoch [45000/200000]: Ntot = 1733.188, Nqll = 450.070, LR = 0.000125\n",
      "Epoch [46000/200000]: Ntot = 407.337, Nqll = 79.624, LR = 0.000125\n",
      "Epoch [47000/200000]: Ntot = 303.759, Nqll = 28.901, LR = 0.000125\n",
      "Epoch [48000/200000]: Ntot = 822.101, Nqll = 78.869, LR = 0.000125\n",
      "Epoch [49000/200000]: Ntot = 103.653, Nqll = 43.654, LR = 0.000125\n",
      "Epoch [50000/200000]: Ntot = 177.726, Nqll = 26.526, LR = 0.000125\n",
      "Epoch [51000/200000]: Ntot = 358.303, Nqll = 44.044, LR = 0.000125\n",
      "Epoch [52000/200000]: Ntot = 333.833, Nqll = 74.352, LR = 0.000125\n",
      "Epoch [53000/200000]: Ntot = 514.154, Nqll = 90.252, LR = 0.000125\n",
      "Epoch [54000/200000]: Ntot = 154.775, Nqll = 30.068, LR = 0.000125\n",
      "Epoch [55000/200000]: Ntot = 426.837, Nqll = 85.155, LR = 6.25e-05\n",
      "Epoch [56000/200000]: Ntot = 333.556, Nqll = 85.271, LR = 6.25e-05\n",
      "Epoch [57000/200000]: Ntot = 485.388, Nqll = 96.771, LR = 6.25e-05\n",
      "Epoch [58000/200000]: Ntot = 594.636, Nqll = 106.574, LR = 6.25e-05\n",
      "Epoch [59000/200000]: Ntot = 549.460, Nqll = 124.185, LR = 6.25e-05\n",
      "Epoch [60000/200000]: Ntot = 466.384, Nqll = 143.000, LR = 6.25e-05\n",
      "Epoch [61000/200000]: Ntot = 689.279, Nqll = 93.367, LR = 6.25e-05\n",
      "Epoch [62000/200000]: Ntot = 173.178, Nqll = 46.048, LR = 6.25e-05\n",
      "Epoch [63000/200000]: Ntot = 31.590, Nqll = 20.654, LR = 6.25e-05\n",
      "Epoch [64000/200000]: Ntot = 44.910, Nqll = 18.699, LR = 6.25e-05\n",
      "Epoch [65000/200000]: Ntot = 157.317, Nqll = 32.140, LR = 3.125e-05\n",
      "Epoch [66000/200000]: Ntot = 190.361, Nqll = 52.149, LR = 3.125e-05\n",
      "Epoch [67000/200000]: Ntot = 239.197, Nqll = 51.279, LR = 3.125e-05\n",
      "Epoch [68000/200000]: Ntot = 281.588, Nqll = 64.808, LR = 3.125e-05\n",
      "Epoch [69000/200000]: Ntot = 453.214, Nqll = 81.175, LR = 3.125e-05\n",
      "Epoch [70000/200000]: Ntot = 741.894, Nqll = 113.310, LR = 3.125e-05\n",
      "Epoch [71000/200000]: Ntot = 909.077, Nqll = 166.936, LR = 3.125e-05\n",
      "Epoch [72000/200000]: Ntot = 542.720, Nqll = 117.674, LR = 3.125e-05\n",
      "Epoch [73000/200000]: Ntot = 460.907, Nqll = 90.133, LR = 3.125e-05\n",
      "Epoch [74000/200000]: Ntot = 512.529, Nqll = 81.204, LR = 3.125e-05\n",
      "Epoch [75000/200000]: Ntot = 673.313, Nqll = 85.220, LR = 1.5625e-05\n",
      "Epoch [76000/200000]: Ntot = 800.465, Nqll = 101.202, LR = 1.5625e-05\n",
      "Epoch [77000/200000]: Ntot = 621.764, Nqll = 93.077, LR = 1.5625e-05\n",
      "Epoch [78000/200000]: Ntot = 550.700, Nqll = 64.584, LR = 1.5625e-05\n",
      "Epoch [79000/200000]: Ntot = 609.421, Nqll = 75.834, LR = 1.5625e-05\n",
      "Epoch [80000/200000]: Ntot = 648.956, Nqll = 81.363, LR = 1.5625e-05\n",
      "Epoch [81000/200000]: Ntot = 717.211, Nqll = 85.188, LR = 1.5625e-05\n",
      "Epoch [82000/200000]: Ntot = 788.196, Nqll = 90.585, LR = 1.5625e-05\n",
      "Epoch [83000/200000]: Ntot = 848.154, Nqll = 94.991, LR = 1.5625e-05\n",
      "Epoch [84000/200000]: Ntot = 901.803, Nqll = 96.755, LR = 1.5625e-05\n",
      "Epoch [85000/200000]: Ntot = 940.280, Nqll = 95.144, LR = 7.8125e-06\n",
      "Epoch [86000/200000]: Ntot = 959.494, Nqll = 92.568, LR = 7.8125e-06\n",
      "Epoch [87000/200000]: Ntot = 973.249, Nqll = 89.255, LR = 7.8125e-06\n",
      "Epoch [88000/200000]: Ntot = 968.002, Nqll = 87.586, LR = 7.8125e-06\n",
      "Epoch [89000/200000]: Ntot = 949.903, Nqll = 88.449, LR = 7.8125e-06\n",
      "Epoch [90000/200000]: Ntot = 942.802, Nqll = 91.463, LR = 7.8125e-06\n",
      "Epoch [91000/200000]: Ntot = 940.287, Nqll = 94.397, LR = 7.8125e-06\n",
      "Epoch [92000/200000]: Ntot = 938.605, Nqll = 96.137, LR = 7.8125e-06\n",
      "Epoch [93000/200000]: Ntot = 936.222, Nqll = 97.030, LR = 7.8125e-06\n",
      "Epoch [94000/200000]: Ntot = 930.646, Nqll = 97.901, LR = 7.8125e-06\n",
      "Epoch [95000/200000]: Ntot = 922.379, Nqll = 98.538, LR = 3.90625e-06\n",
      "Epoch [96000/200000]: Ntot = 916.174, Nqll = 98.849, LR = 3.90625e-06\n",
      "Epoch [97000/200000]: Ntot = 910.613, Nqll = 99.218, LR = 3.90625e-06\n",
      "Epoch [98000/200000]: Ntot = 905.621, Nqll = 99.342, LR = 3.90625e-06\n",
      "Epoch [99000/200000]: Ntot = 842.441, Nqll = 96.056, LR = 3.90625e-06\n",
      "Epoch [100000/200000]: Ntot = 811.296, Nqll = 93.455, LR = 3.90625e-06\n",
      "Epoch [101000/200000]: Ntot = 941.731, Nqll = 105.306, LR = 3.90625e-06\n",
      "Epoch [102000/200000]: Ntot = 897.496, Nqll = 104.318, LR = 3.90625e-06\n",
      "Epoch [103000/200000]: Ntot = 885.428, Nqll = 103.207, LR = 3.90625e-06\n",
      "Epoch [104000/200000]: Ntot = 859.992, Nqll = 100.867, LR = 3.90625e-06\n",
      "Epoch [105000/200000]: Ntot = 848.625, Nqll = 100.134, LR = 1.953125e-06\n",
      "Epoch [106000/200000]: Ntot = 839.553, Nqll = 99.790, LR = 1.953125e-06\n",
      "Epoch [107000/200000]: Ntot = 829.151, Nqll = 99.910, LR = 1.953125e-06\n",
      "Epoch [108000/200000]: Ntot = 818.233, Nqll = 100.341, LR = 1.953125e-06\n",
      "Epoch [109000/200000]: Ntot = 808.477, Nqll = 101.032, LR = 1.953125e-06\n",
      "Epoch [110000/200000]: Ntot = 799.747, Nqll = 101.755, LR = 1.953125e-06\n",
      "Epoch [111000/200000]: Ntot = 792.170, Nqll = 102.439, LR = 1.953125e-06\n",
      "Epoch [112000/200000]: Ntot = 785.969, Nqll = 103.149, LR = 1.953125e-06\n",
      "Epoch [113000/200000]: Ntot = 781.134, Nqll = 103.914, LR = 1.953125e-06\n",
      "Epoch [114000/200000]: Ntot = 777.588, Nqll = 104.738, LR = 1.953125e-06\n",
      "Epoch [115000/200000]: Ntot = 775.212, Nqll = 105.466, LR = 9.765625e-07\n",
      "Epoch [116000/200000]: Ntot = 773.381, Nqll = 105.934, LR = 9.765625e-07\n",
      "Epoch [117000/200000]: Ntot = 771.362, Nqll = 106.439, LR = 9.765625e-07\n",
      "Epoch [118000/200000]: Ntot = 769.655, Nqll = 106.938, LR = 9.765625e-07\n",
      "Epoch [119000/200000]: Ntot = 768.200, Nqll = 107.413, LR = 9.765625e-07\n",
      "Epoch [120000/200000]: Ntot = 767.087, Nqll = 107.868, LR = 9.765625e-07\n",
      "Epoch [121000/200000]: Ntot = 766.443, Nqll = 108.308, LR = 9.765625e-07\n",
      "Epoch [122000/200000]: Ntot = 766.421, Nqll = 108.730, LR = 9.765625e-07\n",
      "Epoch [123000/200000]: Ntot = 767.222, Nqll = 109.118, LR = 9.765625e-07\n",
      "Epoch [124000/200000]: Ntot = 769.101, Nqll = 109.459, LR = 9.765625e-07\n",
      "Epoch [125000/200000]: Ntot = 772.132, Nqll = 109.712, LR = 4.8828125e-07\n",
      "Epoch [126000/200000]: Ntot = 775.025, Nqll = 109.824, LR = 4.8828125e-07\n",
      "Epoch [127000/200000]: Ntot = 779.650, Nqll = 109.940, LR = 4.8828125e-07\n",
      "Epoch [128000/200000]: Ntot = 786.007, Nqll = 110.054, LR = 4.8828125e-07\n",
      "Epoch [129000/200000]: Ntot = 793.972, Nqll = 110.124, LR = 4.8828125e-07\n",
      "Epoch [130000/200000]: Ntot = 803.357, Nqll = 110.161, LR = 4.8828125e-07\n",
      "Epoch [131000/200000]: Ntot = 813.778, Nqll = 110.211, LR = 4.8828125e-07\n",
      "Epoch [132000/200000]: Ntot = 824.882, Nqll = 110.345, LR = 4.8828125e-07\n",
      "Epoch [133000/200000]: Ntot = 836.500, Nqll = 110.637, LR = 4.8828125e-07\n",
      "Epoch [134000/200000]: Ntot = 848.329, Nqll = 111.100, LR = 4.8828125e-07\n",
      "Epoch [135000/200000]: Ntot = 858.387, Nqll = 111.615, LR = 2.44140625e-07\n",
      "Epoch [136000/200000]: Ntot = 863.965, Nqll = 111.908, LR = 2.44140625e-07\n",
      "Epoch [137000/200000]: Ntot = 869.349, Nqll = 112.201, LR = 2.44140625e-07\n",
      "Epoch [138000/200000]: Ntot = 874.393, Nqll = 112.527, LR = 2.44140625e-07\n",
      "Epoch [139000/200000]: Ntot = 878.904, Nqll = 112.912, LR = 2.44140625e-07\n",
      "Epoch [140000/200000]: Ntot = 882.879, Nqll = 113.342, LR = 2.44140625e-07\n",
      "Epoch [141000/200000]: Ntot = 886.426, Nqll = 113.803, LR = 2.44140625e-07\n",
      "Epoch [142000/200000]: Ntot = 889.560, Nqll = 114.302, LR = 2.44140625e-07\n",
      "Epoch [143000/200000]: Ntot = 892.222, Nqll = 114.865, LR = 2.44140625e-07\n",
      "Epoch [144000/200000]: Ntot = 894.318, Nqll = 115.485, LR = 2.44140625e-07\n",
      "Epoch [145000/200000]: Ntot = 895.754, Nqll = 116.036, LR = 1.220703125e-07\n",
      "Epoch [146000/200000]: Ntot = 896.520, Nqll = 116.349, LR = 1.220703125e-07\n",
      "Epoch [147000/200000]: Ntot = 897.202, Nqll = 116.660, LR = 1.220703125e-07\n",
      "Epoch [148000/200000]: Ntot = 897.733, Nqll = 116.967, LR = 1.220703125e-07\n",
      "Epoch [149000/200000]: Ntot = 898.048, Nqll = 117.267, LR = 1.220703125e-07\n",
      "Epoch [150000/200000]: Ntot = 898.146, Nqll = 117.553, LR = 1.220703125e-07\n",
      "Epoch [151000/200000]: Ntot = 898.058, Nqll = 117.825, LR = 1.220703125e-07\n",
      "Epoch [152000/200000]: Ntot = 897.821, Nqll = 118.080, LR = 1.220703125e-07\n",
      "Epoch [153000/200000]: Ntot = 897.498, Nqll = 118.325, LR = 1.220703125e-07\n",
      "Epoch [154000/200000]: Ntot = 897.125, Nqll = 118.564, LR = 1.220703125e-07\n",
      "Epoch [155000/200000]: Ntot = 896.774, Nqll = 118.771, LR = 6.103515625e-08\n",
      "Epoch [156000/200000]: Ntot = 896.568, Nqll = 118.889, LR = 6.103515625e-08\n",
      "Epoch [157000/200000]: Ntot = 896.356, Nqll = 119.006, LR = 6.103515625e-08\n",
      "Epoch [158000/200000]: Ntot = 896.123, Nqll = 119.121, LR = 6.103515625e-08\n",
      "Epoch [159000/200000]: Ntot = 895.870, Nqll = 119.232, LR = 6.103515625e-08\n",
      "Epoch [160000/200000]: Ntot = 895.609, Nqll = 119.342, LR = 6.103515625e-08\n",
      "Epoch [161000/200000]: Ntot = 895.349, Nqll = 119.451, LR = 6.103515625e-08\n",
      "Epoch [162000/200000]: Ntot = 895.093, Nqll = 119.560, LR = 6.103515625e-08\n",
      "Epoch [163000/200000]: Ntot = 894.837, Nqll = 119.668, LR = 6.103515625e-08\n",
      "Epoch [164000/200000]: Ntot = 894.573, Nqll = 119.776, LR = 6.103515625e-08\n",
      "Epoch [165000/200000]: Ntot = 894.327, Nqll = 119.868, LR = 3.0517578125e-08\n",
      "Epoch [166000/200000]: Ntot = 894.183, Nqll = 119.919, LR = 3.0517578125e-08\n",
      "Epoch [167000/200000]: Ntot = 894.036, Nqll = 119.970, LR = 3.0517578125e-08\n",
      "Epoch [168000/200000]: Ntot = 893.877, Nqll = 120.020, LR = 3.0517578125e-08\n",
      "Epoch [169000/200000]: Ntot = 893.705, Nqll = 120.069, LR = 3.0517578125e-08\n",
      "Epoch [170000/200000]: Ntot = 893.524, Nqll = 120.117, LR = 3.0517578125e-08\n",
      "Epoch [171000/200000]: Ntot = 893.341, Nqll = 120.164, LR = 3.0517578125e-08\n",
      "Epoch [172000/200000]: Ntot = 893.154, Nqll = 120.212, LR = 3.0517578125e-08\n",
      "Epoch [173000/200000]: Ntot = 892.963, Nqll = 120.258, LR = 3.0517578125e-08\n",
      "Epoch [174000/200000]: Ntot = 892.768, Nqll = 120.305, LR = 3.0517578125e-08\n",
      "Epoch [175000/200000]: Ntot = 892.592, Nqll = 120.346, LR = 1.52587890625e-08\n",
      "Epoch [176000/200000]: Ntot = 892.491, Nqll = 120.369, LR = 1.52587890625e-08\n",
      "Epoch [177000/200000]: Ntot = 892.387, Nqll = 120.392, LR = 1.52587890625e-08\n",
      "Epoch [178000/200000]: Ntot = 892.279, Nqll = 120.415, LR = 1.52587890625e-08\n",
      "Epoch [179000/200000]: Ntot = 892.166, Nqll = 120.437, LR = 1.52587890625e-08\n",
      "Epoch [180000/200000]: Ntot = 892.052, Nqll = 120.459, LR = 1.52587890625e-08\n",
      "Epoch [181000/200000]: Ntot = 891.937, Nqll = 120.481, LR = 1.52587890625e-08\n",
      "Epoch [182000/200000]: Ntot = 891.821, Nqll = 120.502, LR = 1.52587890625e-08\n",
      "Epoch [183000/200000]: Ntot = 891.704, Nqll = 120.524, LR = 1.52587890625e-08\n",
      "Epoch [184000/200000]: Ntot = 891.583, Nqll = 120.545, LR = 1.52587890625e-08\n",
      "Epoch [185000/200000]: Ntot = 891.455, Nqll = 120.566, LR = 1.52587890625e-08\n",
      "Epoch [186000/200000]: Ntot = 891.320, Nqll = 120.587, LR = 1.52587890625e-08\n",
      "Epoch [187000/200000]: Ntot = 891.182, Nqll = 120.607, LR = 1.52587890625e-08\n",
      "Epoch [188000/200000]: Ntot = 891.043, Nqll = 120.627, LR = 1.52587890625e-08\n",
      "Epoch [189000/200000]: Ntot = 890.904, Nqll = 120.646, LR = 1.52587890625e-08\n",
      "Epoch [190000/200000]: Ntot = 890.765, Nqll = 120.665, LR = 1.52587890625e-08\n",
      "Epoch [191000/200000]: Ntot = 890.624, Nqll = 120.684, LR = 1.52587890625e-08\n",
      "Epoch [192000/200000]: Ntot = 890.481, Nqll = 120.701, LR = 1.52587890625e-08\n",
      "Epoch [193000/200000]: Ntot = 890.335, Nqll = 120.717, LR = 1.52587890625e-08\n",
      "Epoch [194000/200000]: Ntot = 890.190, Nqll = 120.731, LR = 1.52587890625e-08\n",
      "Epoch [195000/200000]: Ntot = 890.047, Nqll = 120.744, LR = 1.52587890625e-08\n",
      "Epoch [196000/200000]: Ntot = 889.906, Nqll = 120.758, LR = 1.52587890625e-08\n",
      "Epoch [197000/200000]: Ntot = 889.766, Nqll = 120.771, LR = 1.52587890625e-08\n",
      "Epoch [198000/200000]: Ntot = 889.627, Nqll = 120.784, LR = 1.52587890625e-08\n",
      "Epoch [199000/200000]: Ntot = 889.490, Nqll = 120.797, LR = 1.52587890625e-08\n",
      "Epoch [200000/200000]: Ntot = 889.354, Nqll = 120.809, LR = 1.52587890625e-08\n",
      "Training complete! Model from epoch 33154 has been saved.\n",
      "Saved model Ntot loss: 14.508.\n",
      "Saved model Nqll loss: 7.819.\n"
     ]
    }
   ],
   "source": [
    "ip.train_IcePINN(\n",
    "    model=model1, \n",
    "    optimizer=optimizer, \n",
    "    training_set=training_set, \n",
    "    epochs=200_000, \n",
    "    name=MODEL_NAME, \n",
    "    print_every=1_000, \n",
    "    print_gradients=False,\n",
    "    LR_scheduler=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train again on smaller LR starting from saved checkpoint (best saved model above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Commencing PINN training on 8320 points for 100000 epochs.\n",
      "Epoch [1000/100000]: Ntot = 23.008, Nqll = 9.562, LR = 0.0001\n",
      "Epoch [2000/100000]: Ntot = 58.882, Nqll = 13.254, LR = 0.0001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m optimizer2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model2\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)\n\u001b[0;32m      6\u001b[0m mn2 \u001b[38;5;241m=\u001b[39m MODEL_NAME\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_round2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_IcePINN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_NAME\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_round2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_gradients\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mLR_scheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\jonescode\\PINN-capstone\\icepinn.py:290\u001b[0m, in \u001b[0;36mtrain_IcePINN\u001b[1;34m(model, optimizer, training_set, epochs, name, print_every, print_gradients, LR_scheduler)\u001b[0m\n\u001b[0;32m    287\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# evaluate collocation point loss\u001b[39;00m\n\u001b[1;32m--> 290\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalc_cp_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_gradients\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    291\u001b[0m Ntot_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(loss[:, \u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    292\u001b[0m Nqll_loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(loss[:, \u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\jonescode\\PINN-capstone\\icepinn.py:241\u001b[0m, in \u001b[0;36mcalc_cp_loss\u001b[1;34m(model, coords, params, epochs, epoch, print_every, print_gradients)\u001b[0m\n\u001b[0;32m    238\u001b[0m ys \u001b[38;5;241m=\u001b[39m enforced_model(coords, model)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# Calculate and extract gradients\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m dNtot_dt, dNqll_dt, dNqll_dxx \u001b[38;5;241m=\u001b[39m \u001b[43mip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (((epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m print_every) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m print_gradients:\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradients: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdNtot_dt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdNqll_dt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdNqll_dxx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\jonescode\\PINN-capstone\\icepinn.py:152\u001b[0m, in \u001b[0;36mcompute_loss_gradients\u001b[1;34m(xs, ys)\u001b[0m\n\u001b[0;32m    148\u001b[0m Nqll_x \u001b[38;5;241m=\u001b[39m grad_Nqll[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    150\u001b[0m \u001b[38;5;66;03m# Compute the second derivative: d²Nqll/dx²\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# We take the gradient of Nqll_x with respect to the initial inputs.\u001b[39;00m\n\u001b[1;32m--> 152\u001b[0m grad_Nqll_x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNqll_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNqll_x\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# I think graph is needed for backprop later\u001b[39;49;00m\n\u001b[0;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaterialize_grads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    160\u001b[0m Nqll_xx \u001b[38;5;241m=\u001b[39m grad_Nqll_x[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Extract ∂²Nqll/∂x² (derivative with respect to x)\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# Return relevant gradients\u001b[39;00m\n",
      "File \u001b[1;32mc:\\jonescode\\PINN-capstone\\venv\\lib\\site-packages\\torch\\autograd\\__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[0;32m    493\u001b[0m         grad_outputs_\n\u001b[0;32m    494\u001b[0m     )\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[0;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[0;32m    509\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\jonescode\\PINN-capstone\\venv\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Instantiate model2 as best version of model1\n",
    "model2 = ip.load_IcePINN(MODEL_NAME)\n",
    "model2.train()\n",
    "optimizer2 = torch.optim.AdamW(model2.parameters(), lr=0.0001)\n",
    "\n",
    "mn2 = MODEL_NAME+\"_round2\"\n",
    "ip.train_IcePINN(\n",
    "    model=model2, \n",
    "    optimizer=optimizer2, \n",
    "    training_set=training_set, \n",
    "    epochs=100_000, \n",
    "    name=MODEL_NAME+\"_round2\", \n",
    "    print_every=1_000, \n",
    "    print_gradients=False,\n",
    "    LR_scheduler=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
